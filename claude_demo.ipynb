{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1e9802",
   "metadata": {},
   "source": [
    "# Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166ca59",
   "metadata": {},
   "source": [
    "To avoid spoilers, run the following commands to close all cell inputs:\n",
    "\n",
    "CTRL + SHIFT + P\n",
    ">Notebook: Collapse All Cell Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd89ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "%pip instal{l anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e424c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .env file with your Claude API key\n",
    "# You can create an API key here: https://console.anthropic.com/\n",
    "# ANTHROPIC_API_KEY=\"your-api-key-here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f521f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables and create your API client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1278e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your first request to Claude\n",
    "message = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is quantum computing? Answer in one sentence\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the text part of the response\n",
    "message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does Claude have built-in context? Let's find out:\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is quantum computing? Answer in one sentence\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write another sentence.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)\n",
    "\n",
    "# What do you think the response of these two requests will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0117af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper functions to make calls easier\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "def chat(messages):\n",
    "    message = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1000,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ada68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with an empty message list\n",
    "messages = []\n",
    "\n",
    "# Add the initial user question\n",
    "add_user_message(messages, \"Define quantum computing in one sentence\")\n",
    "\n",
    "# Get Claude's response\n",
    "answer = chat(messages)\n",
    "print(answer)\n",
    "\n",
    "# Add Claude's response to the conversation history\n",
    "add_assistant_message(messages, answer)\n",
    "\n",
    "# Add a follow-up question\n",
    "add_user_message(messages, \"Write another sentence\")\n",
    "\n",
    "# Get the follow-up response with full context\n",
    "final_answer = chat(messages)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df3955",
   "metadata": {},
   "source": [
    "# System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee18dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable, establish the client, and initialize helper functions\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "# New system parameter added\n",
    "def chat(messages, system=None):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "    # Ensures system prompt is only added if provided\n",
    "    if  system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    # Make the API call with the given parameters\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b925491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example without a system prompt\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(messages,\"How do I solve 5x+3=2 for x?\")\n",
    "answer = chat(messages, system=None)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math Tutor with System Prompt\n",
    "\n",
    "messages = []\n",
    "\n",
    "system = \"\"\"\n",
    "    You are a patient math tutor.\n",
    "    Do not directly answer a student's questions.\n",
    "    Guide them to a solution step by step.\n",
    "    \"\"\"\n",
    "\n",
    "add_user_message(messages,\"How do I solve 5x+3=2 for x?\")\n",
    "answer = chat(messages, system)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt Exercise: How can we add a system prompt to instruct the model to be as concise as possible in its responses?\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(messages,\"Write a Python function that checks a string for duplicate characters.\")\n",
    "answer = chat(messages, system=None)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Solution Here\n",
    "\n",
    "messages = []\n",
    "\n",
    "# Add your system prompt below\n",
    "system = \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "add_user_message(messages,\"Write a Python function that checks a string for duplicate characters.\")\n",
    "answer = chat(messages, system)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d001c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format your output\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution with System Prompt\n",
    "\n",
    "messages = []\n",
    "\n",
    "system = \"\"\"\n",
    "    Your goal is to output a response as concise as possible while still maintaining accuracy. Do not include explanations, only code.\n",
    "    \"\"\"\n",
    "\n",
    "add_user_message(messages,\"Write a Python function that checks a string for duplicate characters.\")\n",
    "answer = chat(messages, system)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b53ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d2491",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ef33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable, establish the client, and initialize helper functions\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "# Set temperature parameter in chat function\n",
    "# Temperature is a parameter that controls the randomness of the model's output\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic\n",
    "def chat(messages, system=None, temperature=1.0):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    if  system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with low temperature\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(messages,\"Generate a one sentence movie idea\")\n",
    "answer = chat(messages, temperature=0.1)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with high temperature\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(messages,\"Generate a one sentence movie idea\")\n",
    "answer = chat(messages, temperature=1.0)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7fd6db",
   "metadata": {},
   "source": [
    "# Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8567aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable, establish the client, and initialize helper functions\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    if  system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "add_user_message(messages, \"Write a 2 sentence description of a fake database\")\n",
    "\n",
    "# Create a streaming response variable\n",
    "stream = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=messages,\n",
    "    # Enable streaming\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bdc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "add_user_message(messages, \"Write a 2 sentence description of a fake database\")\n",
    "\n",
    "with client.messages.stream(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=messages\n",
    ") as stream:\n",
    "    # Iterate over the text stream\n",
    "    for text in stream.text_stream:\n",
    "        # end=\"\" ensures text is printed on the same line without extra newlines\n",
    "        print(text, end=\"\")\n",
    "        pass\n",
    "\n",
    "# Print the final message after streaming\n",
    "stream.get_final_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43135770",
   "metadata": {},
   "source": [
    "# Controlling Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c533423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable, establish the client, and initialize helper functions\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-sonnet-4-0\"\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "# A stop sequence tells the LLM when to stop its response\n",
    "def chat(messages, system=None, temperature=0.2, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        # Include stop sequences if provided\n",
    "        \"stop_sequences\": stop_sequences\n",
    "    }\n",
    "\n",
    "    if  system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without a stop sequence\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(\n",
    "    messages,\n",
    "    \"Count from 1 to 10\"\n",
    ")\n",
    "answer = chat(messages, stop_sequences=[])\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a stop sequence\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(\n",
    "    messages,\n",
    "    \"Count from 1 to 10\"\n",
    ")\n",
    "# Stop the response after reaching 5\n",
    "answer = chat(messages, stop_sequences=[\", 5\"])\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "add_user_message(\n",
    "    messages,\n",
    "    \"Is tea or coffee better at breakfast? One sentence response.\"\n",
    ")\n",
    "\n",
    "answer = chat(messages)\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guiding the model by providing the beginning of its response\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(\n",
    "    messages,\n",
    "    \"Is tea or coffee better at breakfast? One sentence response.\"\n",
    ")\n",
    "\n",
    "add_assistant_message(\n",
    "    messages,\n",
    "    \"Tea is better because\"\n",
    ")\n",
    "\n",
    "answer = chat(messages)\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3869c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we can't modify the system prompt?\n",
    "\n",
    "messages = []\n",
    "\n",
    "add_user_message(\n",
    "    messages,\n",
    "    \"Generate a very short event bridge rule as json\"\n",
    ")\n",
    "add_assistant_message(messages, \"\")\n",
    "text = chat(messages, stop_sequences=[])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "add_user_message(\n",
    "    messages,\n",
    "    \"Generate a very short event bridge rule as json\"\n",
    ")\n",
    "# Start the assistant's response with the opening JSON code block\n",
    "add_assistant_message(messages, \"```json\")\n",
    "# Specify the stop sequence to end the response after the closing code block\n",
    "text = chat(messages, stop_sequences=[\"```\"])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To parse the generated JSON, use the json module\n",
    "\n",
    "import json\n",
    "\n",
    "json.loads(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50323e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Generate 3 AWS CLI commands, nothing else\n",
    "\n",
    "messages = []\n",
    "\n",
    "prompt = \"\"\"\n",
    "Generate three different sample AWS CLI commands. Each should be very short.\n",
    "\"\"\"\n",
    "\n",
    "add_user_message(messages, prompt)\n",
    "\n",
    "text = chat(messages)\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c474491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777dbec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "prompt = \"\"\"\n",
    "Generate three different sample AWS CLI commands. Each should be very short.\n",
    "\"\"\"\n",
    "\n",
    "add_user_message(messages, prompt)\n",
    "\n",
    "text = chat(messages)\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc37e3b",
   "metadata": {},
   "source": [
    "### Example Expected Response\n",
    "aws s3 ls aws ec2 describe-instances aws iam list-users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Solution Here - modify this code block\n",
    "# HINT: Add the add_assistant_message function and a stop sequence to guide the model's output\n",
    "\n",
    "messages = []\n",
    "\n",
    "prompt = \"\"\"\n",
    "Generate three different sample AWS CLI commands. Each should be very short.\n",
    "\"\"\"\n",
    "\n",
    "add_user_message(messages, prompt)\n",
    "\n",
    "text = chat(messages)\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce95e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to format your output to remove \\n\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd63565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "messages = []\n",
    "\n",
    "prompt = \"\"\"\n",
    "Generate three different sample AWS CLI commands. Each should be very short.\n",
    "\"\"\"\n",
    "\n",
    "add_user_message(messages, prompt)\n",
    "add_assistant_message(messages, \"Here are all three commands in a single block without any comments: ```bash\")\n",
    "\n",
    "text = chat(messages, stop_sequences=[\"```\"])\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
